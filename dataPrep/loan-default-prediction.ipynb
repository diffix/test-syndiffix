{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SynDiffix Loan Prediction Tutorial\n",
    "\n",
    "This notebook demonstrates how to use SynDiffix for a loan default prediction model. It is based on the example by Zhou Xu at:\n",
    "\n",
    "https://github.com/zhouxu-ds/loan-default-prediction/blob/main/notebook/modeling.ipynb\n",
    "\n",
    "which builds a model to predict the likelihood of a loan default using the Czech banking dataset.\n",
    "\n",
    "Note that this is not a very good example, for two reasons. First, the dataset is small, and in particular the number of defaulted loans is very small. Second, none of the columns are particularly good features, leading to a low quality model in any event. As a result, different choices in for instance the training and test sets can have a significant effect on the model.\n",
    "\n",
    "### Setup\n",
    "\n",
    "The `syndiffix` package requires Python 3.10 or later. Let's install it and other packages we'll need for the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q syndiffix requests pandas matplotlib numpy seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the datasets\n",
    "\n",
    "These table have all been prepared and loaded onto the open-diffix.org website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bz2\n",
    "import pickle\n",
    "def download_and_load(url):\n",
    "    response = requests.get(url)\n",
    "    data = bz2.decompress(response.content)\n",
    "    df = pickle.loads(data)\n",
    "    return df\n",
    "\n",
    "# Usage\n",
    "df_loan = download_and_load('http://open-diffix.org/datasets/loan.pbz2')\n",
    "df_account = download_and_load('http://open-diffix.org/datasets/account.pbz2')\n",
    "df_district = download_and_load('http://open-diffix.org/datasets/district.pbz2')\n",
    "df_order = download_and_load('http://open-diffix.org/datasets/order.pbz2')\n",
    "df_trans = download_and_load('http://open-diffix.org/datasets/trans.pbz2')\n",
    "df_disp = download_and_load('http://open-diffix.org/datasets/disp.pbz2')\n",
    "df_card = download_and_load('http://open-diffix.org/datasets/card.pbz2')\n",
    "df_client = download_and_load('http://open-diffix.org/datasets/client.pbz2')\n",
    "df_client.rename(columns={'district_id': 'cli_district_id'}, inplace=True)\n",
    "df_card.rename(columns={'type': 'card_type'}, inplace=True)\n",
    "df_account.rename(columns={'date': 'acct_date'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data prep: extract features\n",
    "\n",
    "Zhou extracted a specific set of features that he used for his model. We copy that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['loan_id', 'account_id', 'loan_date', 'amount', 'duration', 'payments',\n",
      "       'status', 'defaulted', 'district_id', 'frequency', 'acct_date', 'city',\n",
      "       'region', 'population', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10',\n",
      "       'avg_salary', 'A12', 'A13', 'entrepreneur_rate', 'A15', 'A16',\n",
      "       'average_unemployment_rate', 'average_crime_rate', 'days_between'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_loan_acct = pd.merge(df_loan, df_account, on='account_id', how='left')\n",
    "df = pd.merge(df_loan_acct, df_district, on='district_id', how='left')\n",
    "df['days_between'] = (df['loan_date'] - df['acct_date']).dt.days\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['loan_id', 'account_id', 'loan_date', 'amount', 'duration', 'payments',\n",
      "       'status', 'defaulted', 'district_id', 'frequency', 'acct_date', 'city',\n",
      "       'region', 'population', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10',\n",
      "       'avg_salary', 'A12', 'A13', 'entrepreneur_rate', 'A15', 'A16',\n",
      "       'average_unemployment_rate', 'average_crime_rate', 'days_between',\n",
      "       'avg_order_amount'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df_order_grouped = df_order.groupby('account_id')['amount'].mean().reset_index()\n",
    "df_order_grouped.rename(columns={'amount': 'avg_order_amount'}, inplace=True)\n",
    "df = pd.merge(df, df_order_grouped, on='account_id', how='left')\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['loan_id', 'account_id', 'loan_date', 'amount', 'duration', 'payments',\n",
      "       'status', 'defaulted', 'district_id', 'frequency', 'acct_date', 'city',\n",
      "       'region', 'population', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10',\n",
      "       'avg_salary', 'A12', 'A13', 'entrepreneur_rate', 'A15', 'A16',\n",
      "       'average_unemployment_rate', 'average_crime_rate', 'days_between',\n",
      "       'avg_order_amount', 'avg_trans_balance', 'avg_trans_amount', 'n_trans'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df_avg_bal = df_trans.groupby('account_id')['balance'].mean().reset_index()\n",
    "df_avg_bal.rename(columns={'balance': 'avg_trans_balance'}, inplace=True)\n",
    "df_avg_amt = df_trans.groupby('account_id')['amount'].mean().reset_index()\n",
    "df_avg_amt.rename(columns={'amount': 'avg_trans_amount'}, inplace=True)\n",
    "df_cnt = df_trans.groupby('account_id').count().iloc[:, 1]\n",
    "df_cnt.name = 'n_trans'\n",
    "df = pd.merge(df, df_avg_bal, on='account_id', how='left')\n",
    "df = pd.merge(df, df_avg_amt, on='account_id', how='left')\n",
    "df = pd.merge(df, df_cnt, on='account_id', how='left')\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['loan_id', 'account_id', 'loan_date', 'amount', 'duration', 'payments',\n",
      "       'status', 'defaulted', 'district_id', 'frequency', 'acct_date', 'city',\n",
      "       'region', 'population', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10',\n",
      "       'avg_salary', 'A12', 'A13', 'entrepreneur_rate', 'A15', 'A16',\n",
      "       'average_unemployment_rate', 'average_crime_rate', 'days_between',\n",
      "       'avg_order_amount', 'avg_trans_balance', 'avg_trans_amount', 'n_trans',\n",
      "       'disp_id', 'client_id', 'type', 'card_id', 'card_type', 'issued'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df_disp_owners = df_disp[df_disp['type'] == 'OWNER']\n",
    "df = pd.merge(df, df_disp_owners, on='account_id', how='left')\n",
    "df = pd.merge(df, df_card, on='disp_id', how='left')\n",
    "df['card_type'].fillna('No', inplace=True)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['loan_id', 'account_id', 'loan_date', 'amount', 'duration', 'payments',\n",
      "       'status', 'defaulted', 'district_id', 'frequency', 'acct_date', 'city',\n",
      "       'region', 'population', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10',\n",
      "       'avg_salary', 'A12', 'A13', 'entrepreneur_rate', 'A15', 'A16',\n",
      "       'average_unemployment_rate', 'average_crime_rate', 'days_between',\n",
      "       'avg_order_amount', 'avg_trans_balance', 'avg_trans_amount', 'n_trans',\n",
      "       'disp_id', 'client_id', 'type', 'card_id', 'card_type', 'issued',\n",
      "       'birth_number', 'cli_district_id', 'sex', 'same_district', 'owner_age'],\n",
      "      dtype='object')\n",
      "682\n"
     ]
    }
   ],
   "source": [
    "df = pd.merge(df, df_client, on='client_id', how='left')\n",
    "df['same_district'] = df['district_id'] == df['cli_district_id']\n",
    "df['owner_age'] = (df['loan_date'] - df['birth_number']).dt.days // 365\n",
    "print(df.columns)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the final feature list selected by Zhou."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "zhou_columns = ['amount', 'duration', 'payments', 'days_between', 'population', \n",
    "            'avg_salary', 'average_unemployment_rate', 'entrepreneur_rate', \n",
    "            'average_crime_rate', 'avg_order_amount', 'avg_trans_amount',\n",
    "            'avg_trans_balance', 'n_trans', 'owner_age', \n",
    "            'frequency', 'card_type', 'same_district', 'sex', 'defaulted']\n",
    "df_ml = df[zhou_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Do the correlations by synthesizing each pair and taking the correlations. Then compare with the original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for synthesis\n",
    "\n",
    "Before we synthesize the data, we need to split the original into training and test dataframes. This is because we will test the synthetic data model against the original test data, and so that data cannot be included in what gets synthesized. Note that the ML model for the original data will have its own train/test split.\n",
    "\n",
    "In addition, we create a dataframe containing identifier for the protected entity. Here the protected entity is the banking account. (Note that in this particular dataset, there is only one loan per account, so this is strictly speaking unecessary. But better safe than sorry.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pid_column = 'account_id'\n",
    "# Add the pid_column back in because SynDiffix needs it\n",
    "df_ml_pid = df[zhou_columns + [pid_column]]\n",
    "\n",
    "df_dx_train = df_ml_pid.sample(n=int(len(df_ml)*0.7), random_state=1)\n",
    "df_dx_test = df_ml_pid.drop(df_dx_train.index)\n",
    "\n",
    "# Separate the pid_column because of the SynDiffix API\n",
    "df_pid = df_dx_train[['account_id']]\n",
    "# And remove it again because it has no analytic value\n",
    "df_dx_train = df_dx_train.drop(columns=[pid_column])\n",
    "df_dx_test = df_dx_test.drop(columns=[pid_column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthesize the data\n",
    "\n",
    "Since we know the target column, we should specify it when we synthesize the data. This will lead to better predictions. There are two options. One is to ask SynDiffix to synthesize every column, and the other is to ask SynDiffix to synthesize only those columns that it determines are good features. We do the latter here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from syndiffix import Synthesizer\n",
    "from syndiffix.clustering.strategy import MlClustering\n",
    "from syndiffix.common import *\n",
    "\n",
    "target_column = 'defaulted'\n",
    "\n",
    "df_syn_feat = Synthesizer(df_dx_train, pids=df_pid,\n",
    "    clustering=MlClustering(target_column=target_column, drop_non_features=True)\n",
    "    #,anonymization_params=AnonymizationParams(salt=bytes([9]))\n",
    "    ).sample()\n",
    "feat_cols = list(df_syn_feat.columns)\n",
    "feat_cols.remove(target_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic data columns without features:\n",
      "['avg_trans_amount', 'avg_trans_balance', 'n_trans']\n",
      "477 (features only) rows, and 477 original data rows\n"
     ]
    }
   ],
   "source": [
    "print(\"Synthetic data columns without non-features:\")\n",
    "print(feat_cols)\n",
    "print(f\"{len(df_syn_feat)} (features only) rows, and {len(df_dx_train)} original data rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listed above are the important features according to SynDiffix.\n",
    "\n",
    "### Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler\n",
    "# Original data transformations\n",
    "num_cols_orig = df_ml.columns[:-5]\n",
    "cat_cols_orig = df_ml.columns[-5:]\n",
    "col_trans_orig = ColumnTransformer([\n",
    "    ('num', MinMaxScaler(), num_cols_orig),\n",
    "    ('cat', OneHotEncoder(drop='if_binary'), cat_cols_orig)\n",
    "])\n",
    "df_transformed_orig = col_trans_orig.fit_transform(df_ml)\n",
    "X_orig = df_transformed_orig[:, :-1]\n",
    "y_orig = df_transformed_orig[:, -1]\n",
    "\n",
    "# For now, we make no transformations on the synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "\n",
    "For the original data, we are using the RandomForestClassifier parameters that Zhou derived in his notebook. We are skipping the steps that he took to decide on thoses parameters.\n",
    "\n",
    "For the synthetic data, we are doing a simple LogisticRegression. Note that we have made no particular effort to fine tune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "# Original\n",
    "# Train test split\n",
    "X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(X_orig, y_orig, test_size=0.3, stratify=y_orig, random_state=10)\n",
    "# Skipping some details here, Zhou settled on the following parameters for his model\n",
    "clf_orig = RandomForestClassifier(n_estimators=10,\n",
    "                             max_depth=None,\n",
    "                             min_samples_split=5,\n",
    "                             min_samples_leaf=1,\n",
    "                             random_state=11)\n",
    "clf_orig.fit(X_train_orig, y_train_orig)\n",
    "y_orig_pred = clf_orig.predict(X_test_orig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Synthetic\n",
    "# Note that df_dx_test is the original data. We test against the original data,\n",
    "# not the synthetic data\n",
    "X_train_syn = df_syn_feat[feat_cols]\n",
    "y_train_syn = df_syn_feat[[target_column]]\n",
    "X_test_syn = df_dx_test[feat_cols]\n",
    "y_test_syn = df_dx_test[[target_column]]\n",
    "y_train_syn_ravel = y_train_syn.values.ravel()\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clr_syn = LogisticRegression()\n",
    "clr_syn.fit(X_train_syn, y_train_syn_ravel)\n",
    "y_syn_pred = clr_syn.predict(X_test_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Acc: 0.8878048780487805\n",
      "Original F1: 0.14814814814814817\n",
      "Synthetic LR Acc: 0.9121951219512195\n",
      "Synthetic LR F1: 0.25\n"
     ]
    }
   ],
   "source": [
    "print('Original Acc:', accuracy_score(y_test_orig, y_orig_pred))\n",
    "print('Original F1:', f1_score(y_test_orig, y_orig_pred))\n",
    "print('Synthetic LR Acc:', accuracy_score(y_test_syn, y_syn_pred))\n",
    "print('Synthetic LR F1:', f1_score(y_test_syn, y_syn_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
