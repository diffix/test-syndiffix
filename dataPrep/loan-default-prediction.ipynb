{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SynDiffix Usage Tutorial\n",
    "\n",
    "This notebook demonstrates how to use SynDiffix for a loan default prediction model. It is based on the example by Zhou Xu at:\n",
    "\n",
    "https://github.com/zhouxu-ds/loan-default-prediction/blob/main/notebook/modeling.ipynb\n",
    "\n",
    "which builds a model to predict the likelihood of a loan default using the Czech banking dataset.\n",
    "\n",
    "### Setup\n",
    "\n",
    "The `syndiffix` package requires Python 3.10 or later. Let's install it and other packages we'll need for the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q syndiffix requests pandas matplotlib numpy seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the datasets\n",
    "\n",
    "These table have all been prepared and loaded onto the open-diffix.org website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bz2\n",
    "import pickle\n",
    "def download_and_load(url):\n",
    "    response = requests.get(url)\n",
    "    data = bz2.decompress(response.content)\n",
    "    df = pickle.loads(data)\n",
    "    return df\n",
    "\n",
    "# Usage\n",
    "df_loan = download_and_load('http://open-diffix.org/datasets/loan.pbz2')\n",
    "df_account = download_and_load('http://open-diffix.org/datasets/account.pbz2')\n",
    "df_district = download_and_load('http://open-diffix.org/datasets/district.pbz2')\n",
    "df_order = download_and_load('http://open-diffix.org/datasets/order.pbz2')\n",
    "df_trans = download_and_load('http://open-diffix.org/datasets/trans.pbz2')\n",
    "df_disp = download_and_load('http://open-diffix.org/datasets/disp.pbz2')\n",
    "df_card = download_and_load('http://open-diffix.org/datasets/card.pbz2')\n",
    "df_client = download_and_load('http://open-diffix.org/datasets/client.pbz2')\n",
    "df_client.rename(columns={'district_id': 'cli_district_id'}, inplace=True)\n",
    "df_card.rename(columns={'type': 'card_type'}, inplace=True)\n",
    "df_account.rename(columns={'date': 'acct_date'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract feature\n",
    "\n",
    "Zhou extracted a specific set of features that he used for his model. We copy that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['loan_id', 'account_id', 'loan_date', 'amount', 'duration', 'payments',\n",
      "       'status', 'defaulted', 'district_id', 'frequency', 'acct_date', 'city',\n",
      "       'region', 'population', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10',\n",
      "       'avg_salary', 'A12', 'A13', 'entrepreneur_rate', 'A15', 'A16',\n",
      "       'average_unemployment_rate', 'average_crime_rate', 'days_between'],\n",
      "      dtype='object')\n",
      "There are 682 loans\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_loan_acct = pd.merge(df_loan, df_account, on='account_id', how='left')\n",
    "df = pd.merge(df_loan_acct, df_district, on='district_id', how='left')\n",
    "df['days_between'] = (df['loan_date'] - df['acct_date']).dt.days\n",
    "print(df.columns)\n",
    "print(f\"There are {len(df)} loans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['loan_id', 'account_id', 'loan_date', 'amount', 'duration', 'payments',\n",
      "       'status', 'defaulted', 'district_id', 'frequency', 'acct_date', 'city',\n",
      "       'region', 'population', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10',\n",
      "       'avg_salary', 'A12', 'A13', 'entrepreneur_rate', 'A15', 'A16',\n",
      "       'average_unemployment_rate', 'average_crime_rate', 'days_between',\n",
      "       'avg_order_amount'],\n",
      "      dtype='object')\n",
      "There are 682 loans\n"
     ]
    }
   ],
   "source": [
    "df_order_grouped = df_order.groupby('account_id')['amount'].mean().reset_index()\n",
    "df_order_grouped.rename(columns={'amount': 'avg_order_amount'}, inplace=True)\n",
    "df = pd.merge(df, df_order_grouped, on='account_id', how='left')\n",
    "print(df.columns)\n",
    "print(f\"There are {len(df)} loans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['loan_id', 'account_id', 'loan_date', 'amount', 'duration', 'payments',\n",
      "       'status', 'defaulted', 'district_id', 'frequency', 'acct_date', 'city',\n",
      "       'region', 'population', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10',\n",
      "       'avg_salary', 'A12', 'A13', 'entrepreneur_rate', 'A15', 'A16',\n",
      "       'average_unemployment_rate', 'average_crime_rate', 'days_between',\n",
      "       'avg_order_amount', 'avg_trans_balance', 'avg_trans_amount', 'n_trans'],\n",
      "      dtype='object')\n",
      "682\n"
     ]
    }
   ],
   "source": [
    "df_avg_bal = df_trans.groupby('account_id')['balance'].mean().reset_index()\n",
    "df_avg_bal.rename(columns={'balance': 'avg_trans_balance'}, inplace=True)\n",
    "df_avg_amt = df_trans.groupby('account_id')['amount'].mean().reset_index()\n",
    "df_avg_amt.rename(columns={'amount': 'avg_trans_amount'}, inplace=True)\n",
    "df_cnt = df_trans.groupby('account_id').count().iloc[:, 1]\n",
    "df_cnt.name = 'n_trans'\n",
    "df = pd.merge(df, df_avg_bal, on='account_id', how='left')\n",
    "df = pd.merge(df, df_avg_amt, on='account_id', how='left')\n",
    "df = pd.merge(df, df_cnt, on='account_id', how='left')\n",
    "print(df.columns)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "card_type\n",
      "No         512\n",
      "classic    133\n",
      "gold        16\n",
      "junior      21\n",
      "Name: account_id, dtype: int64\n",
      "Index(['loan_id', 'account_id', 'loan_date', 'amount', 'duration', 'payments',\n",
      "       'status', 'defaulted', 'district_id', 'frequency', 'acct_date', 'city',\n",
      "       'region', 'population', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10',\n",
      "       'avg_salary', 'A12', 'A13', 'entrepreneur_rate', 'A15', 'A16',\n",
      "       'average_unemployment_rate', 'average_crime_rate', 'days_between',\n",
      "       'avg_order_amount', 'avg_trans_balance', 'avg_trans_amount', 'n_trans',\n",
      "       'disp_id', 'client_id', 'type', 'card_id', 'card_type', 'issued'],\n",
      "      dtype='object')\n",
      "682\n"
     ]
    }
   ],
   "source": [
    "df_disp_owners = df_disp[df_disp['type'] == 'OWNER']\n",
    "df = pd.merge(df, df_disp_owners, on='account_id', how='left')\n",
    "df = pd.merge(df, df_card, on='disp_id', how='left')\n",
    "df['card_type'].fillna('No', inplace=True)\n",
    "print(df.groupby('card_type').count().iloc[:, 1])\n",
    "print(df.columns)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['loan_id', 'account_id', 'loan_date', 'amount', 'duration', 'payments',\n",
      "       'status', 'defaulted', 'district_id', 'frequency', 'acct_date', 'city',\n",
      "       'region', 'population', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10',\n",
      "       'avg_salary', 'A12', 'A13', 'entrepreneur_rate', 'A15', 'A16',\n",
      "       'average_unemployment_rate', 'average_crime_rate', 'days_between',\n",
      "       'avg_order_amount', 'avg_trans_balance', 'avg_trans_amount', 'n_trans',\n",
      "       'disp_id', 'client_id', 'type', 'card_id', 'card_type', 'issued',\n",
      "       'birth_number', 'cli_district_id', 'sex', 'same_district', 'owner_age'],\n",
      "      dtype='object')\n",
      "682\n"
     ]
    }
   ],
   "source": [
    "df = pd.merge(df, df_client, on='client_id', how='left')\n",
    "df['same_district'] = df['district_id'] == df['cli_district_id']\n",
    "df['owner_age'] = (df['loan_date'] - df['birth_number']).dt.days // 365\n",
    "print(df.columns)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the final feature list selected by Zhou."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml = df[['amount', 'duration', 'payments', 'days_between', 'population', \n",
    "            'avg_salary', 'average_unemployment_rate', 'entrepreneur_rate', \n",
    "            'average_crime_rate', 'avg_order_amount', 'avg_trans_amount',\n",
    "            'avg_trans_balance', 'n_trans', 'owner_age', \n",
    "            'frequency', 'card_type', 'same_district', 'sex', 'defaulted']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Do the correlations by synthesizing each pair and taking the correlations. Then compare with the original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for synthesis\n",
    "\n",
    "Before we synthesize the data, we need to split the original into training and test dataframes. This is because we will test the synthetic data model against the original test data, and so that data cannot be included in what gets synthesized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_ml.sample(n=int(len(df_ml)*0.7))\n",
    "df_test = df_ml.drop(df_train.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthesize the data\n",
    "\n",
    "Since we know the target column, we should specify it when we synthesize the data. This will lead to better predictions. There are two options. One is to ask SynDiffix to synthesize every column, and the other is to ask SynDiffix to synthesize only those columns that it determines are good features. We do both here so that we may compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from syndiffix import Synthesizer\n",
    "from syndiffix.clustering.strategy import MlClustering\n",
    "\n",
    "target_column = 'defaulted'\n",
    "df_syn_all = Synthesizer(df_train, target_column=target_column).sample()\n",
    "\n",
    "df_syn_feat = Synthesizer(df_train, clustering=MlClustering(target_column=\"defaulted\", drop_non_features=True)).sample()\n",
    "feat_cols = list(df_syn_feat.columns)\n",
    "feat_cols.remove(target_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, we replicate all transformation and modeling opeations on both the original and synthesized data so that we can compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic data columns without features:\n",
      "Index(['payments', 'avg_trans_amount', 'avg_trans_balance', 'defaulted'], dtype='object') ['payments', 'avg_trans_amount', 'avg_trans_balance']\n",
      "Original data columns:\n",
      "Index(['amount', 'duration', 'payments', 'days_between', 'population',\n",
      "       'avg_salary', 'average_unemployment_rate', 'entrepreneur_rate',\n",
      "       'average_crime_rate', 'avg_order_amount', 'avg_trans_amount',\n",
      "       'avg_trans_balance', 'n_trans', 'owner_age', 'frequency', 'card_type',\n",
      "       'same_district', 'sex', 'defaulted'],\n",
      "      dtype='object')\n",
      "476 (features only) and 476 (all) synthetic data rows, and 682 original data rows\n"
     ]
    }
   ],
   "source": [
    "print(\"Synthetic data columns without features:\")\n",
    "print(df_syn_feat.columns, feat_cols)\n",
    "print(\"Original data columns:\")\n",
    "print(df_train.columns)\n",
    "print(f\"{len(df_syn_feat)} (features only) and {len(df_syn_all)} (all) synthetic data rows, and {len(df_train)} original data rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, we see that SynDiffix found only two columns to be important features.\n",
    "\n",
    "### Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['avg_trans_amount', 'avg_trans_balance'], dtype='object')\n",
      "Index(['defaulted'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler\n",
    "# Original\n",
    "num_cols_orig = df_ml.columns[:-5]\n",
    "cat_cols_orig = df_ml.columns[-5:]\n",
    "col_trans_orig = ColumnTransformer([\n",
    "    ('num', MinMaxScaler(), num_cols_orig),\n",
    "    ('cat', OneHotEncoder(drop='if_binary'), cat_cols_orig)\n",
    "])\n",
    "df_transformed_orig = col_trans_orig.fit_transform(df_ml)\n",
    "X_orig = df_transformed_orig[:, :-1]\n",
    "y_orig = df_transformed_orig[:, -1]\n",
    "\n",
    "#Synthetic (all columns)\n",
    "num_cols_syn_all = df_syn_all.columns[:-5]\n",
    "cat_cols_syn_all = df_syn_all.columns[-5:]\n",
    "col_trans_syn_all = ColumnTransformer([\n",
    "    #('num', MinMaxScaler(), num_cols_syn_all),\n",
    "    ('num', StandardScaler(), num_cols_syn_all),\n",
    "    ('cat', OneHotEncoder(drop='if_binary'), cat_cols_syn_all)\n",
    "])\n",
    "df_transformed_syn_all = col_trans_syn_all.fit_transform(df_syn_all)\n",
    "X_syn_all = df_transformed_syn_all[:, :-1]\n",
    "y_syn_all = df_transformed_syn_all[:, -1]\n",
    "\n",
    "# For now, we're not doing any transformation on the features-only synthetic data\n",
    "X_syn_feat = df_syn_feat[feat_cols]\n",
    "y_syn_feat = df_syn_feat[[target_column]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: Orig: 0.8889473684210525, Syn: 0.8886622807017543\n",
      "F1: Orig: 0.06666666666666668, Syn: 0.0\n",
      "ROC AUC: Orig: 0.7124570919276801, Syn: 0.6939982174688056\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "\n",
    "# Original\n",
    "# Train test split\n",
    "X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(X_orig, y_orig, test_size=0.3, stratify=y_orig, random_state=10)\n",
    "# See the inital model performance\n",
    "clf_orig = RandomForestClassifier(random_state=10)\n",
    "acc_orig = cross_val_score(clf_orig, X_train_orig, y_train_orig, cv=StratifiedKFold(n_splits=5), scoring='accuracy').mean()\n",
    "f1_orig = cross_val_score(clf_orig, X_train_orig, y_train_orig, cv=StratifiedKFold(n_splits=5), scoring='f1').mean()\n",
    "roc_auc_orig = cross_val_score(clf_orig, X_train_orig, y_train_orig, cv=StratifiedKFold(n_splits=5), scoring='roc_auc').mean()\n",
    "\n",
    "# Synthetic\n",
    "# We want to test the synthetic models against the original data. We therefore strictly speaking don't need to split off test data here. Nevertheless, we do it to keep the comparison apples-to-apples.\n",
    "X_train_syn_all, _, y_train_syn_all, _ = train_test_split(X_syn_all, y_syn_all, test_size=0.3, stratify=y_syn_all, random_state=10)\n",
    "# See the inital model performance\n",
    "clf_syn_all = RandomForestClassifier(random_state=10)\n",
    "acc_syn_all = cross_val_score(clf_syn_all, X_train_syn_all, y_train_syn_all, cv=StratifiedKFold(n_splits=5), scoring='accuracy').mean()\n",
    "f1_syn_all = cross_val_score(clf_syn_all, X_train_syn_all, y_train_syn_all, cv=StratifiedKFold(n_splits=5), scoring='f1').mean()\n",
    "roc_auc_syn_all = cross_val_score(clf_syn_all, X_train_syn_all, y_train_syn_all, cv=StratifiedKFold(n_splits=5), scoring='roc_auc').mean()\n",
    "\n",
    "print(f\"Accuracy: Orig: {acc_orig}, Syn: {acc_syn_all}\")\n",
    "print(f\"F1: Orig: {f1_orig}, Syn: {f1_syn_all}\")\n",
    "print(f\"ROC AUC: Orig: {roc_auc_orig}, Syn: {roc_auc_syn_all}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from syndiffix import Synthesizer\n",
    "from syndiffix.clustering.strategy import MlClustering\n",
    "\n",
    "df_syn = Synthesizer(df_ml, clustering=MlClustering(target_column=\"defaulted\", drop_non_features=True)).sample()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
