{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SynDiffix Usage Tutorial\n",
    "\n",
    "This notebook demonstrates how to use SynDiffix for a loan default prediction model. It is based on the example by Zhou Xu at:\n",
    "\n",
    "https://github.com/zhouxu-ds/loan-default-prediction/blob/main/notebook/modeling.ipynb\n",
    "\n",
    "which builds a model to predict the likelihood of a loan default using the Czech banking dataset.\n",
    "\n",
    "### Setup\n",
    "\n",
    "The `syndiffix` package requires Python 3.10 or later. Let's install it and other packages we'll need for the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q syndiffix requests pandas matplotlib numpy seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the datasets\n",
    "\n",
    "These table have all been prepared and loaded onto the open-diffix.org website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bz2\n",
    "import pickle\n",
    "def download_and_load(url):\n",
    "    response = requests.get(url)\n",
    "    data = bz2.decompress(response.content)\n",
    "    df = pickle.loads(data)\n",
    "    return df\n",
    "\n",
    "# Usage\n",
    "df_loan = download_and_load('http://open-diffix.org/datasets/loan.pbz2')\n",
    "df_account = download_and_load('http://open-diffix.org/datasets/account.pbz2')\n",
    "df_district = download_and_load('http://open-diffix.org/datasets/district.pbz2')\n",
    "df_order = download_and_load('http://open-diffix.org/datasets/order.pbz2')\n",
    "df_trans = download_and_load('http://open-diffix.org/datasets/trans.pbz2')\n",
    "df_disp = download_and_load('http://open-diffix.org/datasets/disp.pbz2')\n",
    "df_card = download_and_load('http://open-diffix.org/datasets/card.pbz2')\n",
    "df_client = download_and_load('http://open-diffix.org/datasets/client.pbz2')\n",
    "df_client.rename(columns={'district_id': 'cli_district_id'}, inplace=True)\n",
    "df_card.rename(columns={'type': 'card_type'}, inplace=True)\n",
    "df_account.rename(columns={'date': 'acct_date'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract feature\n",
    "\n",
    "Zhou extracted a specific set of features that he used for his model. We copy that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['loan_id', 'account_id', 'loan_date', 'amount', 'duration', 'payments',\n",
      "       'status', 'defaulted', 'district_id', 'frequency', 'acct_date', 'city',\n",
      "       'region', 'population', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10',\n",
      "       'avg_salary', 'A12', 'A13', 'entrepreneur_rate', 'A15', 'A16',\n",
      "       'average_unemployment_rate', 'average_crime_rate', 'days_between'],\n",
      "      dtype='object')\n",
      "There are 682 loans\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_loan_acct = pd.merge(df_loan, df_account, on='account_id', how='left')\n",
    "df = pd.merge(df_loan_acct, df_district, on='district_id', how='left')\n",
    "df['days_between'] = (df['loan_date'] - df['acct_date']).dt.days\n",
    "print(df.columns)\n",
    "print(f\"There are {len(df)} loans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['loan_id', 'account_id', 'loan_date', 'amount', 'duration', 'payments',\n",
      "       'status', 'defaulted', 'district_id', 'frequency', 'acct_date', 'city',\n",
      "       'region', 'population', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10',\n",
      "       'avg_salary', 'A12', 'A13', 'entrepreneur_rate', 'A15', 'A16',\n",
      "       'average_unemployment_rate', 'average_crime_rate', 'days_between',\n",
      "       'avg_order_amount'],\n",
      "      dtype='object')\n",
      "There are 682 loans\n"
     ]
    }
   ],
   "source": [
    "df_order_grouped = df_order.groupby('account_id')['amount'].mean().reset_index()\n",
    "df_order_grouped.rename(columns={'amount': 'avg_order_amount'}, inplace=True)\n",
    "df = pd.merge(df, df_order_grouped, on='account_id', how='left')\n",
    "print(df.columns)\n",
    "print(f\"There are {len(df)} loans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['loan_id', 'account_id', 'loan_date', 'amount', 'duration', 'payments',\n",
      "       'status', 'defaulted', 'district_id', 'frequency', 'acct_date', 'city',\n",
      "       'region', 'population', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10',\n",
      "       'avg_salary', 'A12', 'A13', 'entrepreneur_rate', 'A15', 'A16',\n",
      "       'average_unemployment_rate', 'average_crime_rate', 'days_between',\n",
      "       'avg_order_amount', 'avg_trans_balance', 'avg_trans_amount', 'n_trans'],\n",
      "      dtype='object')\n",
      "682\n"
     ]
    }
   ],
   "source": [
    "df_avg_bal = df_trans.groupby('account_id')['balance'].mean().reset_index()\n",
    "df_avg_bal.rename(columns={'balance': 'avg_trans_balance'}, inplace=True)\n",
    "df_avg_amt = df_trans.groupby('account_id')['amount'].mean().reset_index()\n",
    "df_avg_amt.rename(columns={'amount': 'avg_trans_amount'}, inplace=True)\n",
    "df_cnt = df_trans.groupby('account_id').count().iloc[:, 1]\n",
    "df_cnt.name = 'n_trans'\n",
    "df = pd.merge(df, df_avg_bal, on='account_id', how='left')\n",
    "df = pd.merge(df, df_avg_amt, on='account_id', how='left')\n",
    "df = pd.merge(df, df_cnt, on='account_id', how='left')\n",
    "print(df.columns)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "card_type\n",
      "No         512\n",
      "classic    133\n",
      "gold        16\n",
      "junior      21\n",
      "Name: account_id, dtype: int64\n",
      "Index(['loan_id', 'account_id', 'loan_date', 'amount', 'duration', 'payments',\n",
      "       'status', 'defaulted', 'district_id', 'frequency', 'acct_date', 'city',\n",
      "       'region', 'population', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10',\n",
      "       'avg_salary', 'A12', 'A13', 'entrepreneur_rate', 'A15', 'A16',\n",
      "       'average_unemployment_rate', 'average_crime_rate', 'days_between',\n",
      "       'avg_order_amount', 'avg_trans_balance', 'avg_trans_amount', 'n_trans',\n",
      "       'disp_id', 'client_id', 'type', 'card_id', 'card_type', 'issued'],\n",
      "      dtype='object')\n",
      "682\n"
     ]
    }
   ],
   "source": [
    "df_disp_owners = df_disp[df_disp['type'] == 'OWNER']\n",
    "df = pd.merge(df, df_disp_owners, on='account_id', how='left')\n",
    "df = pd.merge(df, df_card, on='disp_id', how='left')\n",
    "df['card_type'].fillna('No', inplace=True)\n",
    "print(df.groupby('card_type').count().iloc[:, 1])\n",
    "print(df.columns)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['loan_id', 'account_id', 'loan_date', 'amount', 'duration', 'payments',\n",
      "       'status', 'defaulted', 'district_id', 'frequency', 'acct_date', 'city',\n",
      "       'region', 'population', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10',\n",
      "       'avg_salary', 'A12', 'A13', 'entrepreneur_rate', 'A15', 'A16',\n",
      "       'average_unemployment_rate', 'average_crime_rate', 'days_between',\n",
      "       'avg_order_amount', 'avg_trans_balance', 'avg_trans_amount', 'n_trans',\n",
      "       'disp_id', 'client_id', 'type', 'card_id', 'card_type', 'issued',\n",
      "       'birth_number', 'cli_district_id', 'sex', 'same_district', 'owner_age'],\n",
      "      dtype='object')\n",
      "682\n"
     ]
    }
   ],
   "source": [
    "df = pd.merge(df, df_client, on='client_id', how='left')\n",
    "df['same_district'] = df['district_id'] == df['cli_district_id']\n",
    "df['owner_age'] = (df['loan_date'] - df['birth_number']).dt.days // 365\n",
    "print(df.columns)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the final feature list selected by Zhou."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml = df[['amount', 'duration', 'payments', 'days_between', 'population', \n",
    "            'avg_salary', 'average_unemployment_rate', 'entrepreneur_rate', \n",
    "            'average_crime_rate', 'avg_order_amount', 'avg_trans_amount',\n",
    "            'avg_trans_balance', 'n_trans', 'owner_age', \n",
    "            'frequency', 'card_type', 'same_district', 'sex', 'defaulted']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Do the correlations by synthesizing each pair and taking the correlations. Then compare with the original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for synthesis\n",
    "\n",
    "Before we synthesize the data, we need to split the original into training and test dataframes. This is because we will test the synthetic data model against the original test data, and so that data cannot be included in what gets synthesized. Note that the ML model for the original data will have its own train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dx_train = df_ml.sample(n=int(len(df_ml)*0.7))\n",
    "df_dx_test = df_ml.drop(df_dx_train.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthesize the data\n",
    "\n",
    "Since we know the target column, we should specify it when we synthesize the data. This will lead to better predictions. There are two options. One is to ask SynDiffix to synthesize every column, and the other is to ask SynDiffix to synthesize only those columns that it determines are good features. We do both here so that we may compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from syndiffix import Synthesizer\n",
    "from syndiffix.clustering.strategy import MlClustering\n",
    "\n",
    "target_column = 'defaulted'\n",
    "\n",
    "df_syn_feat = Synthesizer(df_dx_train, clustering=MlClustering(target_column=\"defaulted\", drop_non_features=True)).sample()\n",
    "feat_cols = list(df_syn_feat.columns)\n",
    "feat_cols.remove(target_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, we replicate all transformation and modeling opeations on both the original and synthesized data so that we can compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic data columns without features:\n",
      "['avg_trans_amount', 'avg_trans_balance']\n",
      "477 (features only) rows, and 477 original data rows\n"
     ]
    }
   ],
   "source": [
    "print(\"Synthetic data columns without features:\")\n",
    "print(feat_cols)\n",
    "print(f\"{len(df_syn_feat)} (features only) rows, and {len(df_dx_train)} original data rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, we see that SynDiffix found only two columns to be important features.\n",
    "\n",
    "### Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler\n",
    "# Original\n",
    "num_cols_orig = df_ml.columns[:-5]\n",
    "cat_cols_orig = df_ml.columns[-5:]\n",
    "col_trans_orig = ColumnTransformer([\n",
    "    ('num', MinMaxScaler(), num_cols_orig),\n",
    "    ('cat', OneHotEncoder(drop='if_binary'), cat_cols_orig)\n",
    "])\n",
    "df_transformed_orig = col_trans_orig.fit_transform(df_ml)\n",
    "X_orig = df_transformed_orig[:, :-1]\n",
    "y_orig = df_transformed_orig[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "\n",
    "# Original\n",
    "# Train test split\n",
    "X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(X_orig, y_orig, test_size=0.3, stratify=y_orig, random_state=10)\n",
    "# Skipping some details here, Zhou settled on the following parameters for his model\n",
    "clf_orig = RandomForestClassifier(n_estimators=10,\n",
    "                             max_depth=None,\n",
    "                             min_samples_split=5,\n",
    "                             min_samples_leaf=1,\n",
    "                             random_state=11)\n",
    "clf_orig.fit(X_train_orig, y_train_orig)\n",
    "y_orig_pred = clf_orig.predict(X_test_orig)\n",
    "\n",
    "# Synthetic\n",
    "# For now, we're not doing any transformation on the features-only synthetic data\n",
    "X_train_syn = df_syn_feat[feat_cols]\n",
    "y_train_syn = df_syn_feat[[target_column]]\n",
    "X_test_syn = df_dx_test[feat_cols]\n",
    "y_test_syn = df_dx_test[[target_column]]\n",
    "y_train_syn_ravel = y_train_syn.values.ravel()\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clr_syn = LogisticRegression()\n",
    "clr_syn.fit(X_train_syn, y_train_syn_ravel)\n",
    "y_syn_pred = clr_syn.predict(X_test_syn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [477, 205]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOriginal Train Acc:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train_orig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_orig_pred\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOriginal Train F1:\u001b[39m\u001b[38;5;124m'\u001b[39m, f1_score(y_train_orig, y_orig_pred))\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSynthetic Train Acc:\u001b[39m\u001b[38;5;124m'\u001b[39m, accuracy_score(y_train_syn, y_syn_pred))\n",
      "File \u001b[1;32mc:\\paul\\GitHub\\test-syndiffix\\.conda\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    212\u001b[0m         )\n\u001b[0;32m    213\u001b[0m     ):\n\u001b[1;32m--> 214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    224\u001b[0m     )\n",
      "File \u001b[1;32mc:\\paul\\GitHub\\test-syndiffix\\.conda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:220\u001b[0m, in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;124;03m0.5\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[1;32m--> 220\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\paul\\GitHub\\test-syndiffix\\.conda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:84\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[0;32m     58\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m     \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m     type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     86\u001b[0m     type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\paul\\GitHub\\test-syndiffix\\.conda\\Lib\\site-packages\\sklearn\\utils\\validation.py:407\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    405\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 407\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    408\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    409\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    410\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [477, 205]"
     ]
    }
   ],
   "source": [
    "print('Original Acc:', accuracy_score(y_test_orig, y_orig_pred))\n",
    "print('Original F1:', f1_score(y_test_orig, y_orig_pred))\n",
    "print('Synthetic Acc:', accuracy_score(y_test_syn, y_syn_pred))\n",
    "print('Synthetic F1:', f1_score(y_test_syn, y_syn_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
